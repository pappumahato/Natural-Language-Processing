{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP(Natural Language Processing)\n",
    "##### Natural Language Processing(NLP) is the field of study that deals with understanding, interpreting, and manipulating human languages using computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK: \n",
    "##### NLTK is a powerful python package that contains several algorithms to help computer process, analyze, and understand natural languages and written texts\n",
    "\n",
    "#### NLTK Corpora:\n",
    "##### A corpus is a huge collection of written texts. A compilation of corpuses is called a Corpora. It is a body of written or spoken texts used for linguistic analysis and the development of NLP tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:\n",
    "##### The process of turning a string or a text paragraph into smaller chunks or tokens such as words, phrases, keywords, symbols, etc. is called Tokenization.  (A) sent_tokenize  (B) word_tokenize  (C) RegexpTokenizer  (D) BlanklineTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (A) Sent_tokenize\n",
    "##### To split a body of text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google will shut down its job application tracking system \"Google Hire\" that was launched just two years ago, the company said in a statement.', 'The company had built \"Hire\" with Diane Greene, a former Alphabet board member.', \"Greene left Alphabet's board earlier this year.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sample = \"\"\"Google will shut down its job application tracking system \"Google Hire\" that was launched just two years ago, the company said in a statement.\n",
    "The company had built \"Hire\" with Diane Greene, a former Alphabet board member.\n",
    "Greene left Alphabet's board earlier this year.\"\"\"\n",
    "tokenized_text = sent_tokenize(sample)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One day she was sitting there, all by\\r\\nherself, and it was very still.', 'Suddenly, she heard a little _tap, tap,\\r\\ntap_, at the door.', '\"Who is that?\"', 'she said.', '\"It\\'s the Rain, and I want to come in,\" said a soft, sad, little voice.', '\"No, you can\\'t come in,\" the little Tulip said.', 'By and by she heard another little _tap, tap, tap_ on the window-pane.', '\"Who is there?\"', 'she said.', 'The same soft little voice answered, \"It\\'s the Rain, and I want to come\\r\\nin!\"']\n"
     ]
    }
   ],
   "source": [
    "# Using Gutenberg corpus from nltk corpora\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "sample = gutenberg.raw(\"bryant-stories.txt\")\n",
    "tokenized_text = sent_tokenize(sample)\n",
    "print(tokenized_text[5:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (B) word_tokenize\n",
    "##### To split a body of text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'has', 'finally', 'launched', 'the', 'Google', 'Nest', 'Hub', 'in', 'India', '.', 'The', 'company', 'had', 'first', 'introduced', 'the', 'Google', 'Nest', 'Hub']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample = \"\"\"Google has finally launched the Google Nest Hub in India.\n",
    "The company had first introduced the Google Nest Hub as the Google Home Hub in October last year.\n",
    "And now, atleast a year later google's smart display has finally made its way to India.\"\"\"\n",
    "\n",
    "tok = word_tokenize(sample)\n",
    "\n",
    "print(tok[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.', 'And', 'the', 'earth', 'was', 'without', 'form', ',', 'and', 'void', ';', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep']\n"
     ]
    }
   ],
   "source": [
    "# Using genesis corpus from the nltk corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import genesis\n",
    "\n",
    "sample = genesis.raw(\"english-kjv.txt\")\n",
    "tok = word_tokenize(sample)\n",
    "print(tok[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegexpTokenizer\n",
    "##### To split a string into substring using a regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Google',\n",
       " 'Pixel',\n",
       " 'Made',\n",
       " 'Google',\n",
       " 'Wednesday',\n",
       " 'October',\n",
       " 'The',\n",
       " 'Google',\n",
       " 'Pixel',\n",
       " 'Pixel',\n",
       " 'Watch',\n",
       " 'Google',\n",
       " 'Android']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sample = \"\"\"The Google Pixel 8 series expected to be launched at the 'Made by Google' event 2023 scheduled on Wednesday, October 4.\n",
    "The tech giant is expected to launch a new range of attractive products, including the Google Pixel 8 smartphone series and the Pixel Watch 2. Google is also expected to officially announce its Android 14 operating system at the event.\"\"\"\n",
    "\n",
    "capword_tokenizer = RegexpTokenizer('[A-Z]\\w+')\n",
    "capword_tokenizer.tokenize(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BlanklineTokenizer\n",
    "##### To preserves the empty lines by using the following regular expression '\\s*\\n\\s*\\n*\\s*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88\\nin new York.  Please buy me\\ntwo of them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "\n",
    "sample = \"Good muffins cost $3.88\\nin new York.  Please buy me\\ntwo of them. \\n\\nThanks.\"\n",
    "\n",
    "BlanklineTokenizer().tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That was a very loud beef.',\n",
       " \"I don't even know\\n if this working. Mark?\",\n",
       " 'Mark are you there?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"That was a very loud beef.\\n\\n I don't even know\\n if this working. Mark?\\n\\n Mark are you there?\\n\\n\\n\"\n",
    "\n",
    "BlanklineTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
